---
title: "Part II"
author: "Prithwish Maiti"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
header-includes: \pagenumbering{gobble}
---

\newpage

```{r Setup, include = FALSE}
# Set options
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```

```{r}
# Install relevant libraries
library(tm)
library(slam)
library(knitr)
library(dplyr)
library(data.table)
library(stringr)
library(purrr)
library(pls)
library(caret)
library(naivebayes)
library(MASS)

# Read in GloVe data

glove_data <- read.table("Glove.6B/glove.6B.100d.txt", sep = " ", quote = "", comment.char = "", header = FALSE, stringsAsFactors = FALSE)

# Read in Stanford Sentiment Treebank (SST) data
training_data <- fread("train.csv", header = TRUE, quote = "")
testing_data <- fread("test.csv", header = TRUE, quote = "")
validation_data <- fread("validation.csv", header = TRUE, quote = "")

word_embeddings <- as.data.frame(glove_data)
rownames(word_embeddings) <- word_embeddings$V1
word_embeddings$V1 <- NULL
```

```{r}
# Preprocess function for the data
preprocess_data <- function(text) {
  # Convert to lowercase
  text <- tolower(text)
  # Remove punctuation
  text <- removePunctuation(text)
  # Remove numbers
  text <- removeNumbers(text)
  # Remove extra white spaces
  text <- stripWhitespace(text)
  return(text)
}

# Using word_embeddings to create the final vector
result_matrix_creator <- function(corpus) {
  # Create the document term_list
  document_term_list <- str_extract_all(corpus$content, "[[:alpha:]]+")
  
  result_matrix <- matrix(NA, nrow = length(document_term_list), ncol = ncol(word_embeddings))
  
  for (i in 1:length(document_term_list)) {
    valid_indices <- na.omit(word_embeddings[ document_term_list[[i]] , ])
    if (nrow(valid_indices) > 0) {
      result_matrix[i, ] <- colMeans(valid_indices)
    } else {
      warning(paste("No valid indices for row", i, "in DTM"))
    }
  }
  return(result_matrix)
}

# Function for integrating labels
integrate_labels <- function(result_matrix, original_data_labels) {
  final_data <- as.data.frame(result_matrix) %>% 
    mutate(TwoClass = original_data_labels)
  
  final_data$TwoClass <- as.factor(final_data$TwoClass)
  return(final_data)
}
```

```{r}
training_corpus <- Corpus(VectorSource(training_data$sentence))

# Reprocessing and remove stop words from the corpus
training_corpus <- tm_map(training_corpus, content_transformer(preprocess_data))
training_corpus <- tm_map(training_corpus, removeWords, stopwords("en"))

training_result_matrix <- result_matrix_creator(training_corpus)
final_training_data <- integrate_labels(training_result_matrix, training_data$TwoClass)
final_training_data <- na.omit(final_training_data)
```

```{r}
validation_corpus <- Corpus(VectorSource(validation_data$sentence)) 

# Reprocessing and remove stop words from the corpus
validation_corpus <- tm_map(validation_corpus, content_transformer(preprocess_data))
validation_corpus <- tm_map(validation_corpus, removeWords, stopwords("en"))

validation_result_matrix <- result_matrix_creator(validation_corpus)
final_validation_data <- integrate_labels(validation_result_matrix, validation_data$TwoClass)
final_validation_data <- na.omit(final_validation_data)
```

```{r}
testing_corpus <- Corpus(VectorSource(testing_data$sentence))

# Reprocessing and remove stop words from the corpus
testing_corpus <- tm_map(testing_corpus, content_transformer(preprocess_data))
testing_corpus <- tm_map(testing_corpus, removeWords, stopwords("en"))

testing_result_matrix <- result_matrix_creator(testing_corpus)
final_testing_data <- integrate_labels(testing_result_matrix, testing_data$TwoClass)
final_testing_data <- na.omit(final_testing_data)
```

```{r}

# Cross-Validation 5 times
ctrl <- trainControl(method = "cv", number = 10)
```


# Part II   

The models we have chosen for Part II for two-class prediction of sentence are logistic regression, LDA, QDA, Naive Bayes, Logistic regression on PCA and PLS models. For determining classification performance, we used accuracy within a confusion matrix to determine performance of the models. In terms of hyperparameters,  the PLS model and PCA model were tuned, which required choosing best number of components.   

## Logistic Regression
```{r}
Logistic_Model <- train(TwoClass ~ ., 
                          data = final_training_data, method = "glm", family = "binomial", 
                          trControl = ctrl)


Logisitic_Predict <- predict(Logistic_Model, newdata = final_testing_data)

Logistic_Acc <- confusionMatrix(Logisitic_Predict, final_testing_data$TwoClass)$overall['Accuracy']

Logistic_CM <- confusionMatrix(Logisitic_Predict, final_testing_data$TwoClass)$table
```

The image below depicts the confusion matrix for the logistic regression model to evaluate how it performs on the testing data.
```{r}
Logistic_CM
```

The overall accuracy for the logistic regression model is:
```{r}
Logistic_Acc
```

## Linear Discriminant Analysis
```{r}
LDA_Model <- train(TwoClass ~ ., 
                          data = final_training_data, method = "lda", 
                          trControl = ctrl)
LDA_Predict <- predict(LDA_Model, newdata = final_testing_data)

LDA_Acc <- confusionMatrix(LDA_Predict, final_testing_data$TwoClass)$overall['Accuracy']

LDA_CM <- confusionMatrix(LDA_Predict, final_testing_data$TwoClass)$table
```

The image below depicts the confusion matrix for the LDA model to evaluate how it performs on the testing data.
```{r}
LDA_CM
```

The overall accuracy for the LDA model is:
```{r}
LDA_Acc
```

## Quadratic Discriminant Analysis   
```{r}
QDA_Model <- train(TwoClass ~ ., 
                          data = final_training_data, method = "qda", 
                          trControl = ctrl)
QDA_Predict <- predict(QDA_Model, newdata = final_testing_data)

QDA_Acc <- confusionMatrix(QDA_Predict, final_testing_data$TwoClass)$overall['Accuracy']

QDA_CM <- confusionMatrix(QDA_Predict, final_testing_data$TwoClass)$table
```

The image below depicts the confusion matrix for the QDA model to evaluate how it performs on the testing data.
```{r}
QDA_CM
```

The overall accuracy for the QDA model is:
```{r}
QDA_Acc
```

## Naive Bayes Model   
```{r}
Naive_Bayes_Model <- train(TwoClass ~ ., 
                          data = final_training_data, method = "naive_bayes", 
                          trControl = ctrl)
Naive_Bayes_Predict <- predict(Naive_Bayes_Model, newdata = final_testing_data)

Naive_Bayes_Acc <- confusionMatrix(Naive_Bayes_Predict, final_testing_data$TwoClass)$overall['Accuracy']

Naive_Bayes_CM <- confusionMatrix(Naive_Bayes_Predict, final_testing_data$TwoClass)$table
```

The image below depicts the confusion matrix for the Naive Bayes model to evaluate how it performs on the testing data.
```{r}
Naive_Bayes_CM
```

The overall accuracy for the Naive Bayes model is:
```{r}
Naive_Bayes_Acc
```

## Principal component Classification with logistic regression
Finding the principal component directions from the training set and transforming all the three sets on the basis of these loadings.
```{r}
pc <- prcomp(final_training_data[,-101],
             center = TRUE, scale. = TRUE)
#summary(pc)
train_Z <- as.data.frame(pc$x)
train_Z$TwoClass <- final_training_data$TwoClass
val_Z <- as.data.frame(predict(pc, newdata = final_validation_data[,-101]) )
val_Z$TwoClass <- final_validation_data$TwoClass
test_Z <- as.data.frame(predict(pc, newdata = final_testing_data[,-101]) )
test_Z$TwoClass <- final_testing_data$TwoClass
```

Performing a validation on these components by taking the top PCs.
```{r}
validation_acc <- vector( "numeric" , 50 )
for (i in 1:50) {
  PredictorVariables <- paste("PC", 1:i, sep="")
  Formula <- formula(paste("TwoClass ~ ", 
                           paste(PredictorVariables, collapse=" + ")))
  logistic_reg <- train(Formula, 
                          data = train_Z, method = "glm", family = "binomial")
  
  y_predB <- predict(logistic_reg, newdata = val_Z)
  
  validation_acc[i] <- confusionMatrix(y_predB, val_Z$TwoClass)$overall['Accuracy']
}
plot(validation_acc)
optimal_No_PCs <- which.max(validation_acc)
print(optimal_No_PCs)
```

#testing it
```{r}
PredictorVariables <- paste("PC", 1:optimal_No_PCs, sep="")
Formula <- formula(paste("TwoClass ~ ", 
                         paste(PredictorVariables, collapse=" + ")))
ctrl <- trainControl(method = "cv", number = 10)
PCA_Model <- train(Formula, 
                        data = train_Z, method = "glm", family = "binomial", 
                        trControl = ctrl)

PCA_Predict <- predict(PCA_Model, newdata = test_Z)

PCA_Acc <- confusionMatrix(PCA_Predict, test_Z$TwoClass)$overall['Accuracy']

PCA_CM <- confusionMatrix(PCA_Predict, test_Z$TwoClass)$table
```

The image below depicts the confusion matrix for the PCA validation to evaluate how it performs on the testing data.
```{r}
PCA_CM
```

The overall accuracy for the PCA validation model is:
```{r}
PCA_Acc
```

## Partial Least Squares   

The hyperparameter `ncomp` was used in initial training. The best number of components was determined to be 3. The `predict()` function already uses best tuned model for predicting data.   
```{r}
PLS_Model <- train(TwoClass ~ ., 
                          data = final_training_data, method = "pls", 
                          trControl = ctrl, tunegrid = expand.grid(ncomp = c(1:10)))

PLS_Model$bestTune

PLS_Predict <- predict(PLS_Model, newdata = final_testing_data)

PLS_Acc <- confusionMatrix(PLS_Predict, final_testing_data$TwoClass)$overall['Accuracy']

PLS_CM <- confusionMatrix(PLS_Predict, final_testing_data$TwoClass)$table
```

The image below depicts the confusion matrix for the PLS model to evaluate how it performs on the testing data.
```{r}
PLS_CM
```

The overall accuracy for the PLS model is:
```{r}
PLS_Acc
```

## Results   
```{r}
# Create matrix with 5 columns and 1 row
data <- matrix(c(Logistic_Acc, LDA_Acc, QDA_Acc, Naive_Bayes_Acc,PCA_Acc, PLS_Acc), ncol = 6, byrow = TRUE)
 
# Specify the column names and row names of matrix
colnames(data) <- c("Logistic Reg", "LDA", "QDA", "Naive Bayes", "PCA", "PLS")
rownames(data) <- c('Model accuracy')
 
# Assign to table
Datatable <- kable(round(as.table(data), 3), "simple")
Datatable
```

Based on data table above, the two best models were LDA and PLS, which interestingly both had the same accuracy of 0.7169. The Quadratic Discriminant Analysis had lowest accuracy with 0.6834.   

\newpage   

# Part II Code
```{r ref.label = all_labels(), echo = TRUE, eval = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE}
```