---
title: "Final Project"
author: "Prithwish Maiti"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  \pagenumbering{gobble}
---

\newpage

```{r Setup, include = FALSE}
# Set options
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```

```{r Load and Preprocess Data}
# Install relevant libraries
library(data.table)
library(stringr)
library(purrr)
library(tm)
library(dplyr)
library(rpart)
library(rpart.plot)
library(klaR)
library(caret)
library(randomForest)
library(knitr)
library(gbm)
library(e1071)
# Read in GloVe data

glove_data <- read.table("Glove.6B/glove.6B.100d.txt", sep = " ", quote = "", comment.char = "", header = FALSE, stringsAsFactors = FALSE)

# Read in Stanford Sentiment Treebank (SST) data
training_data <- fread("train.csv", header = TRUE, quote = "")
testing_data <- fread("test.csv", header = TRUE, quote = "")
validation_data <- fread("validation.csv", header = TRUE, quote = "")
```

```{r}
word_embeddings <- as.data.frame(glove_data)
rownames(word_embeddings) <- word_embeddings$V1
word_embeddings$V1 <- NULL

# Preprocess function for the data
preprocess_data <- function(text) {
  # Convert to lowercase
  text <- tolower(text)
  # Remove punctuation
  text <- removePunctuation(text)
  # Remove numbers
  text <- removeNumbers(text)
  # Remove extra white spaces
  text <- stripWhitespace(text)
  return(text)
}

# Using word_embeddings to create the final vector
result_matrix_creator <- function(corpus) {
  # Create the document term_list
  document_term_list <- str_extract_all(corpus$content, "[[:alpha:]]+")
  
  result_matrix <- matrix(NA, nrow = length(document_term_list), ncol = ncol(word_embeddings))
  
  for (i in 1:length(document_term_list)) {
    valid_indices <- na.omit(word_embeddings[ document_term_list[[i]] , ])
    if (nrow(valid_indices) > 0) {
      result_matrix[i, ] <- colMeans(valid_indices)
    } else {
      warning(paste("No valid indices for row", i, "in DTM"))
    }
  }
  return(result_matrix)
}

# Function for integrating labels
integrate_labels <- function(result_matrix, original_data_labels) {
  final_data <- as.data.frame(result_matrix) %>% 
    mutate(FiveClass = original_data_labels)
  
  final_data$FiveClass <- as.factor(final_data$FiveClass)
  return(final_data)
}
```

```{r Training Data}
training_corpus <- Corpus(VectorSource(training_data$sentence))

# Reprocessing and remove stop words from the corpus
training_corpus <- tm_map(training_corpus, content_transformer(preprocess_data))
training_corpus <- tm_map(training_corpus, removeWords, stopwords("en"))

training_result_matrix <- result_matrix_creator(training_corpus)
final_training_data <- integrate_labels(training_result_matrix, training_data$FiveClass)
final_training_data <- na.omit(final_training_data)
```

```{r Validation Data}
validation_corpus <- Corpus(VectorSource(validation_data$sentence))

# Reprocessing and remove stop words from the corpus
validation_corpus <- tm_map(validation_corpus, content_transformer(preprocess_data))
validation_corpus <- tm_map(validation_corpus, removeWords, stopwords("en"))

validation_result_matrix <- result_matrix_creator(validation_corpus)
final_validation_data <- integrate_labels(validation_result_matrix, validation_data$FiveClass)
final_validation_data <- na.omit(final_validation_data)
```

```{r Testing Data}
testing_corpus <- Corpus(VectorSource(testing_data$sentence))

# Reprocessing and remove stop words from the corpus
testing_corpus <- tm_map(testing_corpus, content_transformer(preprocess_data))
testing_corpus <- tm_map(testing_corpus, removeWords, stopwords("en"))

testing_result_matrix <- result_matrix_creator(testing_corpus)
final_testing_data <- integrate_labels(testing_result_matrix, testing_data$FiveClass)
final_testing_data <- na.omit(final_testing_data)
```

# Part III   

The models we have chosen for Part III for five-class prediction of sentence are SVM, pruned classification trees, bagging, random forests, boosting, support vector machines, and DNN models. For determining classification performance, we used accuracy within a confusion matrix to determine performance of the models.    

## Suport Vector Machines
Support vector machines can relax the strict boundary of the linear partition model by incorporation some cost to the mis-classified data points. The total cost must not exceed a prescribed value. Hans Science is making the linear partition flexible to extreme values. 
We have used a linear kernel here.
```{r SVM}
# Set seed for reproducibility
set.seed(10001)

svm_classifier <- svm(FiveClass ~ .,
                      data = final_training_data,
                      type = "C-classification",
                      kernel = "linear",
                      cost = 1.5)

Y_pred <- predict(svm_classifier, final_testing_data[,-101], type = "class")

svm_conf_matrix <- confusionMatrix(final_testing_data$FiveClass, Y_pred)

#Printing the confusion table
svm_conf_matrix$table
```

```{r}
# Print the overall accuracy
svm_conf_matrix$overall[1]
```

## Classification Trees - Pruning   
Classification trees can be a useful method for classification because they are conceptually simple and can be easily interpreted by the general public, but can still be powerful tools for interpretation and prediction.    

The image below depicts a pruned classification tree. We began by creating the largest tree possible with $\alpha=0$ and a `minbucket` of 2. Then, we determined the best value of alpha by consulting the `cptable` and finding the cp associated with the minimum value of `xerror`, the cross-validation error. We then pruned the full tree using the cp value selected in the previous step. Overall, we must consider the bias variance trade-off when selecting a cp value... A small cp results in a larger tree, which means more variance but less bias, and a large cp will result in a very small tree with little variance and high bias.   
```{r Classification Trees - Pruning}
# Set seed for reproducibility
set.seed(10001)

# Create largest tree possible
full_tree <- rpart(FiveClass ~ ., data = final_training_data,
             control = rpart.control( method = "class",
                                      parms = list(split = "information"),
                                      xval = 10,
                                      cp = 0,
                                      minbucket = 2))

# Find the best cp value (alpha)
cp <- full_tree$cptable
cp_min <- which.min(cp[,4])

# Prune the tree using the value selected above
pruned_tree <- prune.rpart(full_tree, cp = cp[cp_min,1])
rpart.plot(pruned_tree)
```

Below is a list of the top 8 most important variables for five-class sentiment classification for the pruned classification tree.
```{r}
# Print a list of top 8 variables of importance
pruned_tree$variable.importance[1:8]
```

The image below depicts the confusion matrix for the pruned classification tree to evaluate how it performs on the testing data.
```{r}
# Evaluate the model on the testing data
predict <- predict(pruned_tree, final_testing_data, type = "class")

# Print the confusion matrix
class_conf_matrix <- confusionMatrix(data = final_testing_data$FiveClass,
                                     reference = predict)
class_conf_matrix$table
```

The overall accuracy for the pruned classification tree is:
```{r}
# Print the overall accuracy
class_conf_matrix$overall[1]
```

One of the disadvantages of a classification tree is that they are not very robust... Changing the seed can drastically change the number of nodes and small changes in the data can have a big impact on the final estimated tree. However, aggregating many decision trees can help address this.   

## Bagging   
Both bagging and random forests are a forms of classification where we build many trees and then combine these in order to obtain one final prediction. They are also known as a general learning method called *ensemble learning*. These methods take the majority vote over all the trees.   

In bagging, we grow deep and complex trees which have high variance, but low bias. However, when we take the majority vote over many trees, the bias stays the same and the variance decreases.   

In the image below, we determine approximately how many trees to use for our bagged model by examining the out-of-bag error rate as the number of trees increases.   

```{r Bagging, out.width = "60%"}
# Set seed for reproducibility
set.seed(10001)

# Bagged model
bagged_1 <- randomForest(FiveClass ~ ., data = final_training_data,
                         mtry = ncol(final_training_data)-1,
                         importance = TRUE)

# Generate plot of oob errors
oob_error <- bagged_1$err.rate[,1]
plot(oob_error, type = "l")
```

Upon reviewing the plot and returning the value for the minimum out-of-bag error, we determine that the optimal value for `ntree` is:   
```{r}
# Identify the number of trees to minimize the oob error rate
min_oob <- which.min(oob_error)
min_oob
```

```{r}
# Set seed for reproducibility
set.seed(10001)

# Final agged model
bagged <- randomForest(FiveClass ~ ., data = final_training_data,
                       ntree = min_oob,
                       mtry = ncol(final_training_data)-1,
                       importance = TRUE)
```

The image below depicts the confusion matrix for the final bagged model to evaluate how it performs on the testing data.
```{r}
# Evaluate the model on the testing data
predict2 <- predict(bagged, final_testing_data, type = "class")

# Print the confusion matrix
bag_conf_matrix <- confusionMatrix(data = final_testing_data$FiveClass,
                                     reference = predict2)
bag_conf_matrix$table
```

The overall accuracy for the final bagged model is:
```{r}
# Print the overall accuracy
bag_conf_matrix$overall[1]
```

## Random Forests   
Random forests is an improvement over bagging because it accounts for the possibility that predictors can be highly correlated. It accomplishes this by using only a subset of the predictors of size *m* at each split. For random forests, we typically select a value of *m* by taking the square root of the number of predictors. Therefore, we will use a value of 10 for `mtry`. We will determine the value of `ntree` using the same process as above for bagging.   

Upon returning the value for the minimum out-of-bag error, we determine that the optimal value for `ntree` is:
```{r Random Forests}
# Set seed for reproducibility
set.seed(10001)

# Random forest model
random_forest_1 <- randomForest(FiveClass ~ ., data = final_training_data,
                                mtry = 10,
                                importance = TRUE)

# Identify the number of trees to minimize the oob error rate
oob_error_2 <- random_forest_1$err.rate[,1]
min_oob_2 <- which.min(oob_error_2)
min_oob_2
```

The image below depicts the confusion matrix for the random forests model to evaluate how it performs on the testing data.
```{r}
# Random forest model
random_forest <- randomForest(FiveClass ~ ., data = final_training_data,
                              ntree = min_oob_2,
                              mtry = 10,
                              importance = TRUE)

# Evaluate the model on the testing data
predict3 <- predict(random_forest, final_testing_data, type = "class")

# Print the confusion matrix
rf_conf_matrix <- confusionMatrix(data = final_testing_data$FiveClass,
                                     reference = predict3)
rf_conf_matrix$table
```

The overall accuracy for the random forests model is:
```{r}
# Print the overall accuracy
rf_conf_matrix$overall[1]
```

## Boosting 
Boosting is also an ensemble learning method, but this method evolves over time. Boosting starts with a weak model and improves its performance by continuing to build new trees sequentially. Each new tree attempts to improve the previous tree by looking at errors.     

The image below depicts the confusion matrix for the random forests model to evaluate how it performs on the testing data.   
```{r Boosting}
# Train the boosting model using gbm
boosted <- gbm(FiveClass ~ ., data = final_training_data, distribution = "multinomial", n.trees = 500, interaction.depth = 3, shrinkage = 0.1)

# Use model to make predictions on test data
pred_test = predict.gbm(object = boosted,
                   newdata = final_testing_data,
                   n.trees = 500,
                   type = "response")

# Give class names to the highest prediction value.
class_names = colnames(pred_test)[apply(pred_test, 1, which.max)]
result = data.frame(final_testing_data$FiveClass, class_names)

# Print the confusion matrix
boost_conf_matrix = confusionMatrix(final_testing_data$FiveClass, as.factor(class_names))
boost_conf_matrix$table
```

The overall accuracy for the boosted model is:
```{r}
# Print the overall accuracy
boost_conf_matrix$overall[1]
```

## Results      
```{r Results}
# Create matrix with 4 columns and 1 row
data <- matrix(c(svm_conf_matrix$overall[1], class_conf_matrix$overall[1], bag_conf_matrix$overall[1], rf_conf_matrix$overall[1], boost_conf_matrix$overall[1]), ncol = 5, byrow = TRUE)
 
# Specify the column names and row names of matrix
colnames(data) = c('SVM_Linear', 'Pruned Classification','Bagged Model','Random Forest','Boosted Model')
rownames(data) <- c('Model accuracy')
 
# Assign to table
final <- kable(round(as.table(data), 3), "simple")
final
```

Unsurprisingly, our pruned classification tree performed worst of these methods, due to the reasons mentioned above. However, the three ensemble learning methods performed very similarly to one another, with random forests ultimately outperforming the others incrementally.   

\newpage   

# Part III Code
```{r ref.label = all_labels(), echo = TRUE, eval = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE}
```
