---
title: "Part I"
author: "Karthik Edupuganti, Robert Ferrand, Rachel Hencher, Prithwish Maiti"
date: "2023-12-09"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

\newpage
```{r Setup, include = FALSE}
# Set options
library(knitr)
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE)
```

```{r}
library(tm)
library(data.table)

#Change your working directory here
setwd("C:/Users/rober/OneDrive/Desktop/563final")
#create training, testing, validation
training_data <- fread("train.csv", header = TRUE, quote = "")
testing_data <- fread("test.csv", header = TRUE, quote = "")
validation_data <- fread("validation.csv", header = TRUE, quote = "")

full_data <- rbind(training_data, testing_data, validation_data)


# Preprocess the data
preprocess_data <- function(text) {
  # Convert to lowercase
  text <- tolower(text)
  # Remove punctuation
  text <- removePunctuation(text)
  # Remove numbers
  text <- removeNumbers(text)
  # Remove extra white spaces
  text <- stripWhitespace(text)
  return(text)
}

# Create the corpus TRAINING
training_corpus <- Corpus(VectorSource(training_data$sentence))
training_corpus <- tm_map(training_corpus, content_transformer(preprocess_data))

# Remove stop words from the corpus
training_corpus <- tm_map(training_corpus, removeWords, stopwords("en"))

# Create DTM (document-term matrix)
training_dtm <- DocumentTermMatrix(training_corpus)
# Remove sparse terms (dimension reduction)
training_dtm_sparse <- removeSparseTerms(training_dtm, 0.9995)
# Convert DTM to matrix format
training_dtm_matrix <- as.matrix(training_dtm_sparse)

#final dataframe training
dtm_matrix <- cbind(training_dtm_matrix, sentiment = training_data$label)
dtm_data <- data.frame(dtm_matrix)


# Create the corpus TESTING
testing_corpus <- Corpus(VectorSource(testing_data$sentence))
testing_corpus <- tm_map(testing_corpus, content_transformer(preprocess_data))

# Remove stop words from the corpus
testing_corpus <- tm_map(testing_corpus, removeWords, stopwords("en"))

# Create DTM (document-term matrix)
testing_dtm <- DocumentTermMatrix(testing_corpus)
# Remove sparse terms (dimension reduction)
testing_dtm_sparse <- removeSparseTerms(testing_dtm, 0.9995)
# Convert DTM to matrix format
testing_dtm_matrix <- as.matrix(testing_dtm_sparse)

#final dataframe testing
dtm_test <- cbind(testing_dtm_matrix, sentiment = testing_data$label)
dtm_data_test <- data.frame(dtm_test)

#now we need to account for dimensionality, add zero rows
zero_rows <- data.frame(matrix(0, nrow = 6334, ncol = ncol(dtm_data_test)))
colnames(zero_rows) <- colnames(dtm_data_test)
test_ready <- rbind(dtm_data_test, zero_rows)

#creating zero rows using concat function and mutate for training
library(dplyr)
concat_train <- dtm_data %>% mutate_all(~ ifelse(. == 1, 0, .))
concat_train <- concat_train[, -ncol(concat_train)]
concat_train[concat_train != 0] <- 0

#creating zero rows using concat function and mutate for testing
library(dplyr)
concat_test <- dtm_data_test %>% mutate_all(~ ifelse(. == 1, 0, .))
concat_test <- concat_test[, -ncol(concat_test)]
concat_test[concat_test != 0] <- 0

zero_rows <- data.frame(matrix(0, nrow = 6334, ncol = ncol(concat_test)))

# Create the corpus VALIDATION
validation_corpus <- Corpus(VectorSource(validation_data$sentence))
validation_corpus <- tm_map(validation_corpus, content_transformer(preprocess_data))

# Remove stop words from the corpus
validation_corpus <- tm_map(validation_corpus, removeWords, stopwords("en"))

# Create DTM (document-term matrix)
validation_dtm <- DocumentTermMatrix(validation_corpus)
# Remove sparse terms (dimension reduction)
validation_dtm_sparse <- removeSparseTerms(validation_dtm, 0.9995)
# Convert DTM to matrix format
validation_dtm_matrix <- as.matrix(validation_dtm_sparse)

dtm_data_validation <- data.frame(validation_dtm_matrix)

#final for validation
concat_validation <- dtm_data_validation %>% mutate_all(~ ifelse(. == 1, 0, .))
concat_validation[concat_validation != 0] <- 0


# Adding zeros to the existing dataset
colnames(zero_rows) <- colnames(concat_test)
concat_testfinal <- rbind(concat_test, zero_rows)

zero_rows2 <- data.frame(matrix(0, nrow = 7443, ncol = ncol(concat_validation)))
colnames(zero_rows2) <- colnames(concat_validation)
concat_validationfinal <- rbind(concat_validation, zero_rows2)


library(purrr)
#all of the common column names are deleted from the concatenated sets
common_cols <- intersect(names(dtm_data), names(concat_testfinal))
concat_testfinal2 <- concat_testfinal[, !names(concat_testfinal) %in% common_cols]

#cbind to combine by column
combined_data <- cbind(dtm_data, concat_testfinal2)

common_cols2 <- intersect(names(combined_data), names(concat_validationfinal))
concat_validationfinal2 <- concat_validationfinal[, !names(concat_validationfinal) %in% common_cols2]

#create the final, actual training set complete with dimensionality
#ready for data analysis
final_training <- cbind(combined_data, concat_validationfinal2)


#same thing as we did with training
newcommon_cols <- intersect(names(test_ready), names(concat_train))
concat_train2 <- concat_train[, !names(concat_train) %in% newcommon_cols]

test_trainsecond <- cbind(test_ready, concat_train2)

common_colsagain <- intersect(names(test_trainsecond), names(concat_validationfinal))
concat_validationfinal2 <- concat_validationfinal[, !names(concat_validationfinal) %in% common_colsagain]


#create the final, actual testing set complete with dimensionality
#ready for data analysis
final_testing <- cbind(test_trainsecond, concat_validationfinal2)


```

# Forward Stepwise
This output shows the top 20 words for both the training and testing sets. We used forward selection due to the large nature of the datasets. Forward selection starts from the null model rather than working from the full model, and adds variables that would best support the model until 20 words are hit. This saved us runtime, and is most practical in this situation. We note that the words selected in each of these models mostly are polarizing, strong, emotional words, such as "best", "wonderful", "worst", "mess", and etc. This seems practical, as those polarizing words tend consistently represent opposite ends of the spectrum (one would not use the word worst to typically describe something positive, and so on).

```{r}
#forward stepwise on the training and testing data
library(leaps)
regfit <- regsubsets(sentiment.1 ~ ., data = final_training, nvmax = 20, method = "forward")
regfit_test <- regsubsets(sentiment.1 ~ ., data = final_testing, nvmax = 20, method = "forward")

#top 20 words are listed
coef(regfit, id=20)
coef(regfit_test, id=20)
```


#Lasso and Ridge Regression 

We ran lasso and ridge regression on both the training and test sets. To find the top words, we compared their magnitude and selected the top 20 most influential words in each model. We see a similar pattern to that of the selection methods, where extremely polarizing words seem to be common choices, such as "crap", "insulting", "amazing", "breathtaking", and etc.

```{r}
library(glmnet)

#create model matrices for test and training to use for lasso and ridge regression
xtrain_c <- model.matrix(sentiment.1 ~ ., final_training)[,-1]
ytrain_c <- final_training$sentiment.1

xtest_c <- model.matrix(sentiment.1 ~ ., final_testing)[,-1]
ytest_c <- final_testing$sentiment.1

#lasso/ridge models run, and print the top 20 words for each train/test set
lasso_train <- cv.glmnet(xtrain_c,ytrain_c,alpha=1)
lasso_coef <- coef(lasso_train)
top_terms_LTrain <- names(sort(abs(lasso_coef[, 1]), decreasing = TRUE)[1:20]);top_terms_LTrain

ridge_train <- cv.glmnet(xtrain_c,ytrain_c,alpha=0)
ridge_coef <- coef(ridge_train)
top_terms_RTrain <- names(sort(abs(ridge_coef[, 1]), decreasing = TRUE)[1:20]);top_terms_RTrain

lasso_test <- cv.glmnet(xtest_c,ytest_c,alpha=1)
lasso_coef_t <- coef(lasso_test)
top_terms_LTest <- names(sort(abs(lasso_coef_t[, 1]), decreasing = TRUE)[1:20]);top_terms_LTest

ridge_test <- cv.glmnet(xtest_c,ytest_c,alpha=0)
ridge_coef_t <- coef(ridge_test)
top_terms_RTest <- names(sort(abs(ridge_coef_t[, 1]), decreasing = TRUE)[1:20]);top_terms_RTest
```

To select the hyperparameter for lasso and ridge, we used the default cross-validation option in the cv.glmnet function. The default folds used is 10, and the lambdas are all chosen very similarly for every model as shown in the given dataframe. With lambdas this low, the penalty term almost has no effect, such that these models are near the classic least squares coefficients.

#Explaining Hyperparameter Lambda
```{r}
#show optimal lambda as hyperparameters
data.frame(Model = c("Lasso_Train", "Lasso_Test", "Ridge_Train", "Ridge_Test"), Lambda = c(lasso_train$lambda.min, lasso_test$lambda.min, ridge_train$lambda.min, ridge_test$lambda.min))
```


#Part I Code
```{r ref.label = all_labels(), echo = TRUE, eval = FALSE, tidy.opts = list(width.cutoff = 70), tidy = TRUE}
```
